# LLM FastAPI Service

This fastapi service was created with the intention for summarisation of transcriptions into a meeting minutes format.

The LLM model is loaded in using the vLLM library.
REST api using FastApi.

## To Run

1. 




TODO list:

- Do README Lmao (How to add in models etc.)
- Implement Speech segment specific chunking rather than brute forceDo not ask any questions.
- Find better prompting methods (Get as concise of a meeting minutes)
- Create a Config.yaml file for changing params (in main.py)
- Doc string for llm_inference_service code